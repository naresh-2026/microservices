
==> Audit <==
|------------|------------------------|----------|--------|---------|---------------------|---------------------|
|  Command   |          Args          | Profile  |  User  | Version |     Start Time      |      End Time       |
|------------|------------------------|----------|--------|---------|---------------------|---------------------|
| start      | --driver=docker        | minikube | naresh | v1.35.0 | 05 Mar 25 02:25 IST |                     |
| start      | --driver=docker        | minikube | naresh | v1.35.0 | 05 Mar 25 02:26 IST | 05 Mar 25 02:27 IST |
| docker-env |                        | minikube | naresh | v1.35.0 | 05 Mar 25 02:48 IST | 05 Mar 25 02:48 IST |
| start      |                        | minikube | naresh | v1.35.0 | 12 Mar 25 23:18 IST |                     |
| start      |                        | minikube | naresh | v1.35.0 | 12 Mar 25 23:21 IST |                     |
| docker-env |                        | minikube | naresh | v1.35.0 | 12 Mar 25 23:24 IST | 12 Mar 25 23:24 IST |
| docker-env |                        | minikube | naresh | v1.35.0 | 12 Mar 25 23:25 IST | 12 Mar 25 23:25 IST |
| stop       |                        | minikube | naresh | v1.35.0 | 12 Mar 25 23:52 IST | 12 Mar 25 23:52 IST |
| start      |                        | minikube | naresh | v1.35.0 | 12 Mar 25 23:52 IST |                     |
| stop       |                        | minikube | naresh | v1.35.0 | 13 Mar 25 00:02 IST | 13 Mar 25 00:02 IST |
| delete     |                        | minikube | naresh | v1.35.0 | 13 Mar 25 00:02 IST | 13 Mar 25 00:02 IST |
| start      |                        | minikube | naresh | v1.35.0 | 13 Mar 25 00:02 IST | 13 Mar 25 00:03 IST |
| service    | go-microservice1 --url | minikube | naresh | v1.35.0 | 13 Mar 25 00:06 IST |                     |
| service    | go-microservice --url  | minikube | naresh | v1.35.0 | 13 Mar 25 00:06 IST |                     |
| service    | go-microservice1 --url | minikube | naresh | v1.35.0 | 13 Mar 25 00:07 IST |                     |
| ip         |                        | minikube | naresh | v1.35.0 | 13 Mar 25 00:11 IST | 13 Mar 25 00:11 IST |
| service    | go-microservice1 --url | minikube | naresh | v1.35.0 | 13 Mar 25 00:11 IST |                     |
| service    | go-microservice1 --url | minikube | naresh | v1.35.0 | 13 Mar 25 00:19 IST |                     |
| service    | go-microservice --url  | minikube | naresh | v1.35.0 | 13 Mar 25 00:20 IST |                     |
| service    | go-microservice1 --url | minikube | naresh | v1.35.0 | 13 Mar 25 00:20 IST |                     |
| service    | go-microservice --url  | minikube | naresh | v1.35.0 | 13 Mar 25 00:21 IST |                     |
|------------|------------------------|----------|--------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/03/13 00:02:57
Running on machine: syn-024-024-000-187
Binary: Built with gc go1.23.4 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0313 00:02:57.470349  114910 out.go:345] Setting OutFile to fd 1 ...
I0313 00:02:57.470713  114910 out.go:397] isatty.IsTerminal(1) = true
I0313 00:02:57.470720  114910 out.go:358] Setting ErrFile to fd 2...
I0313 00:02:57.470726  114910 out.go:397] isatty.IsTerminal(2) = true
I0313 00:02:57.470999  114910 root.go:338] Updating PATH: /home/naresh/.minikube/bin
I0313 00:02:57.471024  114910 oci.go:582] shell is pointing to dockerd inside minikube. will unset to use host
I0313 00:02:57.471640  114910 out.go:352] Setting JSON to false
I0313 00:02:57.475316  114910 start.go:129] hostinfo: {"hostname":"syn-024-024-000-187.inf.spectrum.com","uptime":20457,"bootTime":1741783920,"procs":383,"os":"linux","platform":"fedora","platformFamily":"fedora","platformVersion":"41","kernelVersion":"6.13.5-200.fc41.x86_64","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"ab48cea6-0420-4c6c-99a8-cf063cb7db1f"}
I0313 00:02:57.475496  114910 start.go:139] virtualization: kvm host
I0313 00:02:57.480292  114910 out.go:177] üòÑ  minikube v1.35.0 on Fedora 41
I0313 00:02:57.483528  114910 out.go:177]     ‚ñ™ MINIKUBE_ACTIVE_DOCKERD=minikube
I0313 00:02:57.483666  114910 notify.go:220] Checking for updates...
I0313 00:02:57.485615  114910 driver.go:394] Setting default libvirt URI to qemu:///system
I0313 00:02:57.485669  114910 global.go:112] Querying for installed drivers using PATH=/home/naresh/.minikube/bin:/home/naresh/.local/bin:/home/naresh/bin:/usr/local/bin:/usr/local/sbin:/usr/bin:/usr/sbin:/opt/flutter/bin:/opt/flutter/bin:/opt/flutter/bin/cache/dart-sdk/bin:/home/naresh/flutter/bin:/home/naresh/flutter/bin/cache/dart-sdk/bin:/opt/flutter/bin:/home/naresh/flutter/bin:/home/naresh/flutter/bin/cache/dart-sdk/bin:/opt/flutter/bin
I0313 00:02:57.486110  114910 global.go:133] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I0313 00:02:57.486299  114910 global.go:133] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in $PATH Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I0313 00:02:57.531756  114910 docker.go:123] docker version: linux-27.3.1:
I0313 00:02:57.531999  114910 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0313 00:02:57.616403  114910 info.go:266] docker info: {ID:0bab8512-09f9-4cbb-b9b1-c75ea8f255db Containers:3 ContainersRunning:0 ContainersPaused:0 ContainersStopped:3 Images:3 Driver:overlay2 DriverStatus:[[Backing Filesystem btrfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:23 OomKillDisable:false NGoroutines:43 SystemTime:2025-03-13 00:02:57.599591684 +0530 IST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.13.5-200.fc41.x86_64 OperatingSystem:Fedora Linux 41 (Workstation Edition) OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:7995539456 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:syn-024-024-000-187.inf.spectrum.com Labels:[] ExperimentalBuild:false ServerVersion:27.3.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:/usr/bin/tini-static ContainerdCommit:{ID:1.fc41 Expected:1.fc41} RuncCommit:{ID: Expected:} InitCommit:{ID: Expected:} SecurityOptions:[name=seccomp,profile=builtin name=selinux name=cgroupns] ProductLicense: Warnings:[WARNING: bridge-nf-call-iptables is disabled WARNING: bridge-nf-call-ip6tables is disabled] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:0.18.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:2.30.3]] Warnings:<nil>}}
I0313 00:02:57.616548  114910 docker.go:318] overlay module found
I0313 00:02:57.616575  114910 global.go:133] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0313 00:02:57.649118  114910 global.go:133] none default: false priority: 4, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:running the 'none' driver as a regular user requires sudo permissions Reason: Fix: Doc: Version:}
W0313 00:02:57.660573  114910 podman.go:138] podman returned error: exit status 1
I0313 00:02:57.660623  114910 global.go:133] podman default: true priority: 7, state: {Installed:true Healthy:false Running:false NeedsImprovement:false Error:"sudo -n -k podman version --format {{.Version}}" exit status 1: sudo: a password is required Reason: Fix:Add your user to the 'sudoers' file: 'naresh ALL=(ALL) NOPASSWD: /usr/bin/podman' , or run 'minikube config set rootless true' Doc:https://podman.io Version:}
I0313 00:02:57.660687  114910 global.go:133] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0313 00:02:58.305889  114910 global.go:133] kvm2 default: true priority: 8, state: {Installed:true Healthy:true Running:true NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0313 00:02:58.308585  114910 global.go:133] qemu2 default: true priority: 7, state: {Installed:true Healthy:true Running:true NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0313 00:02:58.308613  114910 driver.go:316] not recommending "ssh" due to default: false
I0313 00:02:58.308619  114910 driver.go:311] not recommending "podman" due to health: "sudo -n -k podman version --format {{.Version}}" exit status 1: sudo: a password is required
I0313 00:02:58.308640  114910 driver.go:351] Picked: docker
I0313 00:02:58.308650  114910 driver.go:352] Alternatives: [kvm2 qemu2 ssh]
I0313 00:02:58.308657  114910 driver.go:353] Rejects: [virtualbox vmware none podman]
I0313 00:02:58.310544  114910 out.go:177] ‚ú®  Automatically selected the docker driver. Other choices: kvm2, qemu2, ssh
I0313 00:02:58.311919  114910 start.go:297] selected driver: docker
I0313 00:02:58.311931  114910 start.go:901] validating driver "docker" against <nil>
I0313 00:02:58.311945  114910 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0313 00:02:58.312047  114910 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0313 00:02:58.394530  114910 info.go:266] docker info: {ID:0bab8512-09f9-4cbb-b9b1-c75ea8f255db Containers:3 ContainersRunning:0 ContainersPaused:0 ContainersStopped:3 Images:3 Driver:overlay2 DriverStatus:[[Backing Filesystem btrfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:23 OomKillDisable:false NGoroutines:43 SystemTime:2025-03-13 00:02:58.379372809 +0530 IST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.13.5-200.fc41.x86_64 OperatingSystem:Fedora Linux 41 (Workstation Edition) OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:7995539456 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:syn-024-024-000-187.inf.spectrum.com Labels:[] ExperimentalBuild:false ServerVersion:27.3.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:/usr/bin/tini-static ContainerdCommit:{ID:1.fc41 Expected:1.fc41} RuncCommit:{ID: Expected:} InitCommit:{ID: Expected:} SecurityOptions:[name=seccomp,profile=builtin name=selinux name=cgroupns] ProductLicense: Warnings:[WARNING: bridge-nf-call-iptables is disabled WARNING: bridge-nf-call-ip6tables is disabled] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:0.18.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:2.30.3]] Warnings:<nil>}}
I0313 00:02:58.394767  114910 start_flags.go:310] no existing cluster config was found, will generate one from the flags 
I0313 00:02:58.396907  114910 start_flags.go:393] Using suggested 2200MB memory alloc based on sys=7625MB, container=7625MB
I0313 00:02:58.397342  114910 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]
I0313 00:02:58.399974  114910 out.go:177] üìå  Using Docker driver with root privileges
I0313 00:02:58.402171  114910 cni.go:84] Creating CNI manager for ""
I0313 00:02:58.402254  114910 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0313 00:02:58.402268  114910 start_flags.go:319] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0313 00:02:58.402369  114910 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/naresh:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0313 00:02:58.405820  114910 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0313 00:02:58.407532  114910 cache.go:121] Beginning downloading kic base image for docker with docker
I0313 00:02:58.410291  114910 out.go:177] üöú  Pulling base image v0.0.46 ...
I0313 00:02:58.412101  114910 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0313 00:02:58.412201  114910 preload.go:146] Found local preload: /home/naresh/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0313 00:02:58.412215  114910 cache.go:56] Caching tarball of preloaded images
I0313 00:02:58.412223  114910 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0313 00:02:58.413139  114910 preload.go:172] Found /home/naresh/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0313 00:02:58.413160  114910 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0313 00:02:58.413709  114910 profile.go:143] Saving config to /home/naresh/.minikube/profiles/minikube/config.json ...
I0313 00:02:58.413744  114910 lock.go:35] WriteFile acquiring /home/naresh/.minikube/profiles/minikube/config.json: {Name:mk68f48aa7e15656fb80322503a8dd5b5c102de3 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0313 00:02:58.442598  114910 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon, skipping pull
I0313 00:02:58.442615  114910 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 exists in daemon, skipping load
I0313 00:02:58.442635  114910 cache.go:227] Successfully downloaded all kic artifacts
I0313 00:02:58.442667  114910 start.go:360] acquireMachinesLock for minikube: {Name:mk9dc8ee575afcee7b1e5a59d7ec77b8eddf2d9f Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0313 00:02:58.442742  114910 start.go:364] duration metric: took 57.166¬µs to acquireMachinesLock for "minikube"
I0313 00:02:58.442766  114910 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/naresh:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0313 00:02:58.442874  114910 start.go:125] createHost starting for "" (driver="docker")
I0313 00:02:58.444657  114910 out.go:235] üî•  Creating docker container (CPUs=2, Memory=2200MB) ...
I0313 00:02:58.445114  114910 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0313 00:02:58.445143  114910 client.go:168] LocalClient.Create starting
I0313 00:02:58.445264  114910 main.go:141] libmachine: Reading certificate data from /home/naresh/.minikube/certs/ca.pem
I0313 00:02:58.445317  114910 main.go:141] libmachine: Decoding PEM data...
I0313 00:02:58.445335  114910 main.go:141] libmachine: Parsing certificate...
I0313 00:02:58.445425  114910 main.go:141] libmachine: Reading certificate data from /home/naresh/.minikube/certs/cert.pem
I0313 00:02:58.445456  114910 main.go:141] libmachine: Decoding PEM data...
I0313 00:02:58.445469  114910 main.go:141] libmachine: Parsing certificate...
I0313 00:02:58.446030  114910 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0313 00:02:58.465713  114910 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0313 00:02:58.465815  114910 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I0313 00:02:58.465839  114910 cli_runner.go:164] Run: docker network inspect minikube
W0313 00:02:58.485637  114910 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0313 00:02:58.485659  114910 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0313 00:02:58.485690  114910 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0313 00:02:58.485838  114910 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0313 00:02:58.505709  114910 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc0015eaed0}
I0313 00:02:58.505757  114910 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0313 00:02:58.505822  114910 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0313 00:02:58.802197  114910 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0313 00:02:58.802230  114910 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0313 00:02:58.802488  114910 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0313 00:02:58.827103  114910 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0313 00:02:58.851646  114910 oci.go:103] Successfully created a docker volume minikube
I0313 00:02:58.851739  114910 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -d /var/lib
I0313 00:02:59.699149  114910 oci.go:107] Successfully prepared a docker volume minikube
I0313 00:02:59.699184  114910 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0313 00:02:59.699205  114910 kic.go:194] Starting extracting preloaded images to volume ...
I0313 00:02:59.699453  114910 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/naresh/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -I lz4 -xf /preloaded.tar -C /extractDir
W0313 00:03:00.172609  114910 cli_runner.go:211] docker run --rm --entrypoint /usr/bin/tar -v /home/naresh/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -I lz4 -xf /preloaded.tar -C /extractDir returned with exit code 2
I0313 00:03:00.172658  114910 kic.go:201] Unable to extract preloaded tarball to volume: docker run --rm --entrypoint /usr/bin/tar -v /home/naresh/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -I lz4 -xf /preloaded.tar -C /extractDir: exit status 2
stdout:

stderr:
tar (child): /preloaded.tar: Cannot open: Permission denied
tar (child): Error is not recoverable: exiting now
/usr/bin/tar: Child returned status 2
/usr/bin/tar: Error is not recoverable: exiting now
W0313 00:03:00.172920  114910 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
W0313 00:03:00.172945  114910 oci.go:249] Your kernel does not support CPU cfs period/quota or the cgroup is not mounted.
I0313 00:03:00.173010  114910 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0313 00:03:00.235870  114910 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2200mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279
I0313 00:03:00.740867  114910 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0313 00:03:00.766466  114910 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0313 00:03:00.792893  114910 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0313 00:03:00.862025  114910 oci.go:144] the created container "minikube" has a running status.
I0313 00:03:00.862072  114910 kic.go:225] Creating ssh key for kic: /home/naresh/.minikube/machines/minikube/id_rsa...
I0313 00:03:01.073538  114910 kic_runner.go:191] docker (temp): /home/naresh/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0313 00:03:01.136487  114910 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0313 00:03:01.176247  114910 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0313 00:03:01.176270  114910 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0313 00:03:01.250614  114910 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0313 00:03:01.279066  114910 machine.go:93] provisionDockerMachine start ...
I0313 00:03:01.279162  114910 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0313 00:03:01.302083  114910 main.go:141] libmachine: Using SSH client type: native
I0313 00:03:01.302339  114910 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32778 <nil> <nil>}
I0313 00:03:01.302351  114910 main.go:141] libmachine: About to run SSH command:
hostname
I0313 00:03:01.464254  114910 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0313 00:03:01.464276  114910 ubuntu.go:169] provisioning hostname "minikube"
I0313 00:03:01.464363  114910 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0313 00:03:01.489811  114910 main.go:141] libmachine: Using SSH client type: native
I0313 00:03:01.490312  114910 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32778 <nil> <nil>}
I0313 00:03:01.490327  114910 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0313 00:03:01.667082  114910 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0313 00:03:01.667188  114910 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0313 00:03:01.694370  114910 main.go:141] libmachine: Using SSH client type: native
I0313 00:03:01.694660  114910 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32778 <nil> <nil>}
I0313 00:03:01.694682  114910 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0313 00:03:01.842565  114910 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0313 00:03:01.842590  114910 ubuntu.go:175] set auth options {CertDir:/home/naresh/.minikube CaCertPath:/home/naresh/.minikube/certs/ca.pem CaPrivateKeyPath:/home/naresh/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/naresh/.minikube/machines/server.pem ServerKeyPath:/home/naresh/.minikube/machines/server-key.pem ClientKeyPath:/home/naresh/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/naresh/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/naresh/.minikube}
I0313 00:03:01.842634  114910 ubuntu.go:177] setting up certificates
I0313 00:03:01.842648  114910 provision.go:84] configureAuth start
I0313 00:03:01.842739  114910 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0313 00:03:01.865027  114910 provision.go:143] copyHostCerts
I0313 00:03:01.865099  114910 exec_runner.go:144] found /home/naresh/.minikube/ca.pem, removing ...
I0313 00:03:01.865110  114910 exec_runner.go:203] rm: /home/naresh/.minikube/ca.pem
I0313 00:03:01.865330  114910 exec_runner.go:151] cp: /home/naresh/.minikube/certs/ca.pem --> /home/naresh/.minikube/ca.pem (1078 bytes)
I0313 00:03:01.865511  114910 exec_runner.go:144] found /home/naresh/.minikube/cert.pem, removing ...
I0313 00:03:01.865518  114910 exec_runner.go:203] rm: /home/naresh/.minikube/cert.pem
I0313 00:03:01.865569  114910 exec_runner.go:151] cp: /home/naresh/.minikube/certs/cert.pem --> /home/naresh/.minikube/cert.pem (1119 bytes)
I0313 00:03:01.865671  114910 exec_runner.go:144] found /home/naresh/.minikube/key.pem, removing ...
I0313 00:03:01.865676  114910 exec_runner.go:203] rm: /home/naresh/.minikube/key.pem
I0313 00:03:01.865722  114910 exec_runner.go:151] cp: /home/naresh/.minikube/certs/key.pem --> /home/naresh/.minikube/key.pem (1675 bytes)
I0313 00:03:01.865812  114910 provision.go:117] generating server cert: /home/naresh/.minikube/machines/server.pem ca-key=/home/naresh/.minikube/certs/ca.pem private-key=/home/naresh/.minikube/certs/ca-key.pem org=naresh.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0313 00:03:01.955420  114910 provision.go:177] copyRemoteCerts
I0313 00:03:01.955548  114910 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0313 00:03:01.955584  114910 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0313 00:03:01.973249  114910 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32778 SSHKeyPath:/home/naresh/.minikube/machines/minikube/id_rsa Username:docker}
I0313 00:03:02.079472  114910 ssh_runner.go:362] scp /home/naresh/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I0313 00:03:02.118000  114910 ssh_runner.go:362] scp /home/naresh/.minikube/machines/server.pem --> /etc/docker/server.pem (1180 bytes)
I0313 00:03:02.148287  114910 ssh_runner.go:362] scp /home/naresh/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0313 00:03:02.180461  114910 provision.go:87] duration metric: took 337.796897ms to configureAuth
I0313 00:03:02.180486  114910 ubuntu.go:193] setting minikube options for container-runtime
I0313 00:03:02.180736  114910 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0313 00:03:02.180811  114910 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0313 00:03:02.201838  114910 main.go:141] libmachine: Using SSH client type: native
I0313 00:03:02.202162  114910 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32778 <nil> <nil>}
I0313 00:03:02.202176  114910 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0313 00:03:02.348740  114910 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0313 00:03:02.348758  114910 ubuntu.go:71] root file system type: overlay
I0313 00:03:02.348915  114910 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0313 00:03:02.349002  114910 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0313 00:03:02.371196  114910 main.go:141] libmachine: Using SSH client type: native
I0313 00:03:02.371475  114910 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32778 <nil> <nil>}
I0313 00:03:02.371576  114910 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0313 00:03:02.544175  114910 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0313 00:03:02.544291  114910 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0313 00:03:02.566016  114910 main.go:141] libmachine: Using SSH client type: native
I0313 00:03:02.566287  114910 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x8641c0] 0x866ea0 <nil>  [] 0s} 127.0.0.1 32778 <nil> <nil>}
I0313 00:03:02.566306  114910 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0313 00:03:04.151930  114910 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-12-17 15:44:19.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-03-12 18:33:02.540918275 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0313 00:03:04.151960  114910 machine.go:96] duration metric: took 2.872880172s to provisionDockerMachine
I0313 00:03:04.151975  114910 client.go:171] duration metric: took 5.706826263s to LocalClient.Create
I0313 00:03:04.151995  114910 start.go:167] duration metric: took 5.7068821s to libmachine.API.Create "minikube"
I0313 00:03:04.152003  114910 start.go:293] postStartSetup for "minikube" (driver="docker")
I0313 00:03:04.152016  114910 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0313 00:03:04.152090  114910 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0313 00:03:04.152141  114910 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0313 00:03:04.173982  114910 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32778 SSHKeyPath:/home/naresh/.minikube/machines/minikube/id_rsa Username:docker}
I0313 00:03:04.285744  114910 ssh_runner.go:195] Run: cat /etc/os-release
I0313 00:03:04.290521  114910 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0313 00:03:04.290550  114910 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0313 00:03:04.290561  114910 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0313 00:03:04.290568  114910 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0313 00:03:04.290579  114910 filesync.go:126] Scanning /home/naresh/.minikube/addons for local assets ...
I0313 00:03:04.290648  114910 filesync.go:126] Scanning /home/naresh/.minikube/files for local assets ...
I0313 00:03:04.290679  114910 start.go:296] duration metric: took 138.669784ms for postStartSetup
I0313 00:03:04.291143  114910 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0313 00:03:04.313973  114910 profile.go:143] Saving config to /home/naresh/.minikube/profiles/minikube/config.json ...
I0313 00:03:04.314475  114910 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0313 00:03:04.314543  114910 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0313 00:03:04.335580  114910 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32778 SSHKeyPath:/home/naresh/.minikube/machines/minikube/id_rsa Username:docker}
I0313 00:03:04.434898  114910 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0313 00:03:04.441283  114910 start.go:128] duration metric: took 5.998382956s to createHost
I0313 00:03:04.441300  114910 start.go:83] releasing machines lock for "minikube", held for 5.99854845s
I0313 00:03:04.441377  114910 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0313 00:03:04.463278  114910 ssh_runner.go:195] Run: cat /version.json
I0313 00:03:04.463308  114910 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0313 00:03:04.463331  114910 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0313 00:03:04.463384  114910 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0313 00:03:04.485114  114910 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32778 SSHKeyPath:/home/naresh/.minikube/machines/minikube/id_rsa Username:docker}
I0313 00:03:04.485975  114910 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32778 SSHKeyPath:/home/naresh/.minikube/machines/minikube/id_rsa Username:docker}
I0313 00:03:09.713695  114910 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (5.250342741s)
I0313 00:03:09.713748  114910 ssh_runner.go:235] Completed: cat /version.json: (5.250444482s)
W0313 00:03:09.713796  114910 start.go:867] [curl -sS -m 2 https://registry.k8s.io/] failed: curl -sS -m 2 https://registry.k8s.io/: Process exited with status 28
stdout:

stderr:
curl: (28) Resolving timed out after 2000 milliseconds
I0313 00:03:09.714162  114910 ssh_runner.go:195] Run: systemctl --version
I0313 00:03:09.723354  114910 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0313 00:03:09.731975  114910 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0313 00:03:09.779077  114910 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0313 00:03:09.779159  114910 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0313 00:03:09.812287  114910 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0313 00:03:09.812310  114910 start.go:495] detecting cgroup driver to use...
I0313 00:03:09.812351  114910 detect.go:190] detected "systemd" cgroup driver on host os
I0313 00:03:09.812472  114910 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0313 00:03:09.836343  114910 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0313 00:03:09.852751  114910 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0313 00:03:09.865878  114910 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0313 00:03:09.865955  114910 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
W0313 00:03:09.875727  114910 out.go:270] ‚ùó  Failing to connect to https://registry.k8s.io/ from inside the minikube container
W0313 00:03:09.875785  114910 out.go:270] üí°  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0313 00:03:09.881745  114910 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0313 00:03:09.896558  114910 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0313 00:03:09.914266  114910 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0313 00:03:09.928997  114910 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0313 00:03:09.943481  114910 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0313 00:03:09.960634  114910 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0313 00:03:09.975372  114910 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0313 00:03:09.988780  114910 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0313 00:03:10.000238  114910 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0313 00:03:10.011244  114910 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0313 00:03:10.096066  114910 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0313 00:03:10.211657  114910 start.go:495] detecting cgroup driver to use...
I0313 00:03:10.211713  114910 detect.go:190] detected "systemd" cgroup driver on host os
I0313 00:03:10.211786  114910 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0313 00:03:10.230333  114910 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0313 00:03:10.230431  114910 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0313 00:03:10.251963  114910 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0313 00:03:10.282336  114910 ssh_runner.go:195] Run: which cri-dockerd
I0313 00:03:10.287985  114910 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0313 00:03:10.308729  114910 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0313 00:03:10.341959  114910 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0313 00:03:10.430677  114910 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0313 00:03:10.515460  114910 docker.go:574] configuring docker to use "systemd" as cgroup driver...
I0313 00:03:10.515588  114910 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0313 00:03:10.543215  114910 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0313 00:03:10.632702  114910 ssh_runner.go:195] Run: sudo systemctl restart docker
I0313 00:03:12.212892  114910 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.580155064s)
I0313 00:03:12.212987  114910 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0313 00:03:12.230443  114910 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0313 00:03:12.247740  114910 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0313 00:03:12.340330  114910 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0313 00:03:12.428949  114910 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0313 00:03:12.505919  114910 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0313 00:03:12.537553  114910 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0313 00:03:12.554968  114910 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0313 00:03:12.643233  114910 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0313 00:03:12.743769  114910 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0313 00:03:12.743891  114910 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0313 00:03:12.749273  114910 start.go:563] Will wait 60s for crictl version
I0313 00:03:12.749398  114910 ssh_runner.go:195] Run: which crictl
I0313 00:03:12.754205  114910 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0313 00:03:12.793719  114910 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0313 00:03:12.793797  114910 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0313 00:03:12.822261  114910 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0313 00:03:12.852819  114910 out.go:235] üê≥  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
I0313 00:03:12.853032  114910 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0313 00:03:12.875193  114910 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0313 00:03:12.880293  114910 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0313 00:03:12.894755  114910 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/naresh:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0313 00:03:12.894887  114910 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0313 00:03:12.894948  114910 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0313 00:03:12.917090  114910 docker.go:689] Got preloaded images: 
I0313 00:03:12.917108  114910 docker.go:695] registry.k8s.io/kube-apiserver:v1.32.0 wasn't preloaded
I0313 00:03:12.917175  114910 ssh_runner.go:195] Run: sudo cat /var/lib/docker/image/overlay2/repositories.json
I0313 00:03:12.928929  114910 ssh_runner.go:195] Run: which lz4
I0313 00:03:12.933307  114910 ssh_runner.go:195] Run: stat -c "%s %y" /preloaded.tar.lz4
I0313 00:03:12.937578  114910 ssh_runner.go:352] existence check for /preloaded.tar.lz4: stat -c "%s %y" /preloaded.tar.lz4: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/preloaded.tar.lz4': No such file or directory
I0313 00:03:12.937601  114910 ssh_runner.go:362] scp /home/naresh/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 --> /preloaded.tar.lz4 (349777613 bytes)
I0313 00:03:14.038353  114910 docker.go:653] duration metric: took 1.105081011s to copy over tarball
I0313 00:03:14.038431  114910 ssh_runner.go:195] Run: sudo tar --xattrs --xattrs-include security.capability -I lz4 -C /var -xf /preloaded.tar.lz4
I0313 00:03:20.137468  114910 ssh_runner.go:235] Completed: sudo tar --xattrs --xattrs-include security.capability -I lz4 -C /var -xf /preloaded.tar.lz4: (6.098996165s)
I0313 00:03:20.137503  114910 ssh_runner.go:146] rm: /preloaded.tar.lz4
I0313 00:03:20.520671  114910 ssh_runner.go:195] Run: sudo cat /var/lib/docker/image/overlay2/repositories.json
I0313 00:03:20.533584  114910 ssh_runner.go:362] scp memory --> /var/lib/docker/image/overlay2/repositories.json (2631 bytes)
I0313 00:03:20.651941  114910 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0313 00:03:20.896160  114910 ssh_runner.go:195] Run: sudo systemctl restart docker
I0313 00:03:23.722244  114910 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.826050609s)
I0313 00:03:23.722344  114910 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0313 00:03:23.751440  114910 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0313 00:03:23.751459  114910 cache_images.go:84] Images are preloaded, skipping loading
I0313 00:03:23.751472  114910 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0313 00:03:23.751588  114910 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0313 00:03:23.751661  114910 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0313 00:03:23.827928  114910 cni.go:84] Creating CNI manager for ""
I0313 00:03:23.827974  114910 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0313 00:03:23.827988  114910 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0313 00:03:23.828015  114910 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0313 00:03:23.828163  114910 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0313 00:03:23.828245  114910 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0313 00:03:23.852530  114910 binaries.go:44] Found k8s binaries, skipping transfer
I0313 00:03:23.852625  114910 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0313 00:03:23.867260  114910 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0313 00:03:23.892086  114910 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0313 00:03:23.916691  114910 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2285 bytes)
I0313 00:03:23.942662  114910 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0313 00:03:23.949899  114910 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0313 00:03:23.968429  114910 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0313 00:03:24.050555  114910 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0313 00:03:24.084310  114910 certs.go:68] Setting up /home/naresh/.minikube/profiles/minikube for IP: 192.168.49.2
I0313 00:03:24.084328  114910 certs.go:194] generating shared ca certs ...
I0313 00:03:24.084352  114910 certs.go:226] acquiring lock for ca certs: {Name:mkb4b7409da0d29caa78f73ecb525f6c8e501779 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0313 00:03:24.084825  114910 certs.go:235] skipping valid "minikubeCA" ca cert: /home/naresh/.minikube/ca.key
I0313 00:03:24.084921  114910 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/naresh/.minikube/proxy-client-ca.key
I0313 00:03:24.084939  114910 certs.go:256] generating profile certs ...
I0313 00:03:24.085040  114910 certs.go:363] generating signed profile cert for "minikube-user": /home/naresh/.minikube/profiles/minikube/client.key
I0313 00:03:24.085056  114910 crypto.go:68] Generating cert /home/naresh/.minikube/profiles/minikube/client.crt with IP's: []
I0313 00:03:24.274663  114910 crypto.go:156] Writing cert to /home/naresh/.minikube/profiles/minikube/client.crt ...
I0313 00:03:24.274681  114910 lock.go:35] WriteFile acquiring /home/naresh/.minikube/profiles/minikube/client.crt: {Name:mk1cf65eff9cc678f11e7cc3635b69d68432c344 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0313 00:03:24.274928  114910 crypto.go:164] Writing key to /home/naresh/.minikube/profiles/minikube/client.key ...
I0313 00:03:24.274934  114910 lock.go:35] WriteFile acquiring /home/naresh/.minikube/profiles/minikube/client.key: {Name:mkef4bba24efeba28d32d939992a5ddb40a4fd3d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0313 00:03:24.275008  114910 certs.go:363] generating signed profile cert for "minikube": /home/naresh/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0313 00:03:24.275018  114910 crypto.go:68] Generating cert /home/naresh/.minikube/profiles/minikube/apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0313 00:03:24.459448  114910 crypto.go:156] Writing cert to /home/naresh/.minikube/profiles/minikube/apiserver.crt.7fb57e3c ...
I0313 00:03:24.459465  114910 lock.go:35] WriteFile acquiring /home/naresh/.minikube/profiles/minikube/apiserver.crt.7fb57e3c: {Name:mk87b5ab0223fda9ca268a205d02ad6c4d1250a9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0313 00:03:24.459666  114910 crypto.go:164] Writing key to /home/naresh/.minikube/profiles/minikube/apiserver.key.7fb57e3c ...
I0313 00:03:24.459671  114910 lock.go:35] WriteFile acquiring /home/naresh/.minikube/profiles/minikube/apiserver.key.7fb57e3c: {Name:mkc3f552bdd7815778b07a6af7a98621c41e7600 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0313 00:03:24.459770  114910 certs.go:381] copying /home/naresh/.minikube/profiles/minikube/apiserver.crt.7fb57e3c -> /home/naresh/.minikube/profiles/minikube/apiserver.crt
I0313 00:03:24.460181  114910 certs.go:385] copying /home/naresh/.minikube/profiles/minikube/apiserver.key.7fb57e3c -> /home/naresh/.minikube/profiles/minikube/apiserver.key
I0313 00:03:24.460424  114910 certs.go:363] generating signed profile cert for "aggregator": /home/naresh/.minikube/profiles/minikube/proxy-client.key
I0313 00:03:24.460436  114910 crypto.go:68] Generating cert /home/naresh/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0313 00:03:24.591075  114910 crypto.go:156] Writing cert to /home/naresh/.minikube/profiles/minikube/proxy-client.crt ...
I0313 00:03:24.591085  114910 lock.go:35] WriteFile acquiring /home/naresh/.minikube/profiles/minikube/proxy-client.crt: {Name:mk66f8d435290ae02020bc9a34b776842f825a89 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0313 00:03:24.591200  114910 crypto.go:164] Writing key to /home/naresh/.minikube/profiles/minikube/proxy-client.key ...
I0313 00:03:24.591205  114910 lock.go:35] WriteFile acquiring /home/naresh/.minikube/profiles/minikube/proxy-client.key: {Name:mke509f6cce8cd34ebead5ac24f47d2ce74eb6f7 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0313 00:03:24.591340  114910 certs.go:484] found cert: /home/naresh/.minikube/certs/ca-key.pem (1675 bytes)
I0313 00:03:24.591363  114910 certs.go:484] found cert: /home/naresh/.minikube/certs/ca.pem (1078 bytes)
I0313 00:03:24.591379  114910 certs.go:484] found cert: /home/naresh/.minikube/certs/cert.pem (1119 bytes)
I0313 00:03:24.591395  114910 certs.go:484] found cert: /home/naresh/.minikube/certs/key.pem (1675 bytes)
I0313 00:03:24.591926  114910 ssh_runner.go:362] scp /home/naresh/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0313 00:03:24.630546  114910 ssh_runner.go:362] scp /home/naresh/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0313 00:03:24.664863  114910 ssh_runner.go:362] scp /home/naresh/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0313 00:03:24.696264  114910 ssh_runner.go:362] scp /home/naresh/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0313 00:03:24.728864  114910 ssh_runner.go:362] scp /home/naresh/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0313 00:03:24.762448  114910 ssh_runner.go:362] scp /home/naresh/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0313 00:03:24.793954  114910 ssh_runner.go:362] scp /home/naresh/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0313 00:03:24.828510  114910 ssh_runner.go:362] scp /home/naresh/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0313 00:03:24.870152  114910 ssh_runner.go:362] scp /home/naresh/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0313 00:03:24.916624  114910 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0313 00:03:24.944261  114910 ssh_runner.go:195] Run: openssl version
I0313 00:03:24.967842  114910 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0313 00:03:24.998442  114910 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0313 00:03:25.004997  114910 certs.go:528] hashing: -rw-r--r--. 1 root root 1111 Mar  4 20:57 /usr/share/ca-certificates/minikubeCA.pem
I0313 00:03:25.005094  114910 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0313 00:03:25.014607  114910 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0313 00:03:25.028063  114910 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0313 00:03:25.034232  114910 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0313 00:03:25.034285  114910 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/naresh:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0313 00:03:25.034414  114910 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0313 00:03:25.056252  114910 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0313 00:03:25.068738  114910 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0313 00:03:25.080282  114910 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0313 00:03:25.080343  114910 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0313 00:03:25.101930  114910 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0313 00:03:25.101949  114910 kubeadm.go:157] found existing configuration files:

I0313 00:03:25.102041  114910 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0313 00:03:25.116136  114910 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0313 00:03:25.116202  114910 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0313 00:03:25.126891  114910 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0313 00:03:25.139109  114910 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0313 00:03:25.139169  114910 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0313 00:03:25.150395  114910 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0313 00:03:25.163221  114910 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0313 00:03:25.163286  114910 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0313 00:03:25.174786  114910 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0313 00:03:25.186445  114910 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0313 00:03:25.186507  114910 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0313 00:03:25.197760  114910 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.32.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0313 00:03:25.279003  114910 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0313 00:03:25.386498  114910 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0313 00:03:36.023646  114910 kubeadm.go:310] [init] Using Kubernetes version: v1.32.0
I0313 00:03:36.023725  114910 kubeadm.go:310] [preflight] Running pre-flight checks
I0313 00:03:36.023891  114910 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0313 00:03:36.024046  114910 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0313 00:03:36.024200  114910 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0313 00:03:36.024310  114910 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0313 00:03:36.025981  114910 out.go:235]     ‚ñ™ Generating certificates and keys ...
I0313 00:03:36.026276  114910 kubeadm.go:310] [certs] Using existing ca certificate authority
I0313 00:03:36.026383  114910 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0313 00:03:36.026489  114910 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I0313 00:03:36.026584  114910 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I0313 00:03:36.026689  114910 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I0313 00:03:36.026780  114910 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I0313 00:03:36.026877  114910 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I0313 00:03:36.027144  114910 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0313 00:03:36.027237  114910 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I0313 00:03:36.027445  114910 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0313 00:03:36.027561  114910 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I0313 00:03:36.027664  114910 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I0313 00:03:36.027731  114910 kubeadm.go:310] [certs] Generating "sa" key and public key
I0313 00:03:36.027841  114910 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0313 00:03:36.027936  114910 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0313 00:03:36.028026  114910 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0313 00:03:36.028111  114910 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0313 00:03:36.028203  114910 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0313 00:03:36.028290  114910 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0313 00:03:36.028411  114910 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0313 00:03:36.028521  114910 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0313 00:03:36.030290  114910 out.go:235]     ‚ñ™ Booting up control plane ...
I0313 00:03:36.030476  114910 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0313 00:03:36.030667  114910 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0313 00:03:36.030796  114910 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0313 00:03:36.031075  114910 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0313 00:03:36.031285  114910 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0313 00:03:36.031366  114910 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0313 00:03:36.031636  114910 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0313 00:03:36.031861  114910 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0313 00:03:36.031998  114910 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 501.018147ms
I0313 00:03:36.032158  114910 kubeadm.go:310] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0313 00:03:36.032271  114910 kubeadm.go:310] [api-check] The API server is healthy after 6.003669273s
I0313 00:03:36.032499  114910 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0313 00:03:36.032771  114910 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0313 00:03:36.032904  114910 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0313 00:03:36.033214  114910 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0313 00:03:36.033322  114910 kubeadm.go:310] [bootstrap-token] Using token: pv3o6c.qsfl6i5exiisa42j
I0313 00:03:36.034568  114910 out.go:235]     ‚ñ™ Configuring RBAC rules ...
I0313 00:03:36.034758  114910 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0313 00:03:36.035036  114910 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0313 00:03:36.035274  114910 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0313 00:03:36.035463  114910 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0313 00:03:36.035690  114910 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0313 00:03:36.035954  114910 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0313 00:03:36.036174  114910 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0313 00:03:36.036235  114910 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0313 00:03:36.036298  114910 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0313 00:03:36.036303  114910 kubeadm.go:310] 
I0313 00:03:36.036413  114910 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0313 00:03:36.036421  114910 kubeadm.go:310] 
I0313 00:03:36.036598  114910 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0313 00:03:36.036606  114910 kubeadm.go:310] 
I0313 00:03:36.036653  114910 kubeadm.go:310]   mkdir -p $HOME/.kube
I0313 00:03:36.036752  114910 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0313 00:03:36.036823  114910 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0313 00:03:36.036829  114910 kubeadm.go:310] 
I0313 00:03:36.036922  114910 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0313 00:03:36.036928  114910 kubeadm.go:310] 
I0313 00:03:36.036998  114910 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0313 00:03:36.037004  114910 kubeadm.go:310] 
I0313 00:03:36.037085  114910 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0313 00:03:36.037197  114910 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0313 00:03:36.037292  114910 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0313 00:03:36.037296  114910 kubeadm.go:310] 
I0313 00:03:36.037414  114910 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0313 00:03:36.037544  114910 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0313 00:03:36.037554  114910 kubeadm.go:310] 
I0313 00:03:36.037691  114910 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token pv3o6c.qsfl6i5exiisa42j \
I0313 00:03:36.037837  114910 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:1f6db1df5807ce65f127d6fdedf0e2fea0a730439ab452106aaa2464f257b679 \
I0313 00:03:36.037875  114910 kubeadm.go:310] 	--control-plane 
I0313 00:03:36.037880  114910 kubeadm.go:310] 
I0313 00:03:36.037996  114910 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0313 00:03:36.038005  114910 kubeadm.go:310] 
I0313 00:03:36.038115  114910 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token pv3o6c.qsfl6i5exiisa42j \
I0313 00:03:36.038283  114910 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:1f6db1df5807ce65f127d6fdedf0e2fea0a730439ab452106aaa2464f257b679 
I0313 00:03:36.038295  114910 cni.go:84] Creating CNI manager for ""
I0313 00:03:36.038314  114910 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0313 00:03:36.041335  114910 out.go:177] üîó  Configuring bridge CNI (Container Networking Interface) ...
I0313 00:03:36.043201  114910 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0313 00:03:36.059795  114910 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0313 00:03:36.094639  114910 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0313 00:03:36.094756  114910 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0313 00:03:36.094880  114910 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_03_13T00_03_36_0700 minikube.k8s.io/version=v1.35.0 minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0313 00:03:36.114503  114910 ops.go:34] apiserver oom_adj: -16
I0313 00:03:36.214338  114910 kubeadm.go:1113] duration metric: took 119.695162ms to wait for elevateKubeSystemPrivileges
I0313 00:03:36.245328  114910 kubeadm.go:394] duration metric: took 11.211032494s to StartCluster
I0313 00:03:36.245381  114910 settings.go:142] acquiring lock: {Name:mk52fea8b14255955c8d7c94c858b6849f9e5138 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0313 00:03:36.245548  114910 settings.go:150] Updating kubeconfig:  /home/naresh/.kube/config
I0313 00:03:36.246815  114910 lock.go:35] WriteFile acquiring /home/naresh/.kube/config: {Name:mkbd42c5664eab7bc9f8b06a8cdb99f92a1d8a9a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0313 00:03:36.247803  114910 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0313 00:03:36.247811  114910 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0313 00:03:36.247901  114910 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0313 00:03:36.248039  114910 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0313 00:03:36.248059  114910 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0313 00:03:36.248071  114910 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I0313 00:03:36.248059  114910 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0313 00:03:36.248095  114910 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0313 00:03:36.248121  114910 host.go:66] Checking if "minikube" exists ...
I0313 00:03:36.248807  114910 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0313 00:03:36.248947  114910 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0313 00:03:36.249887  114910 out.go:177] üîé  Verifying Kubernetes components...
I0313 00:03:36.251700  114910 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0313 00:03:36.291826  114910 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0313 00:03:36.294361  114910 addons.go:238] Setting addon default-storageclass=true in "minikube"
I0313 00:03:36.294385  114910 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0313 00:03:36.294404  114910 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0313 00:03:36.294423  114910 host.go:66] Checking if "minikube" exists ...
I0313 00:03:36.294548  114910 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0313 00:03:36.295568  114910 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0313 00:03:36.347372  114910 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32778 SSHKeyPath:/home/naresh/.minikube/machines/minikube/id_rsa Username:docker}
I0313 00:03:36.347550  114910 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0313 00:03:36.347594  114910 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0313 00:03:36.347733  114910 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0313 00:03:36.392064  114910 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32778 SSHKeyPath:/home/naresh/.minikube/machines/minikube/id_rsa Username:docker}
I0313 00:03:36.423762  114910 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.49.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0313 00:03:36.437783  114910 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0313 00:03:36.503103  114910 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0313 00:03:36.526956  114910 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0313 00:03:36.676480  114910 start.go:971] {"host.minikube.internal": 192.168.49.1} host record injected into CoreDNS's ConfigMap
I0313 00:03:36.677942  114910 api_server.go:52] waiting for apiserver process to appear ...
I0313 00:03:36.678048  114910 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0313 00:03:36.900754  114910 api_server.go:72] duration metric: took 652.824003ms to wait for apiserver process to appear ...
I0313 00:03:36.900772  114910 api_server.go:88] waiting for apiserver healthz status ...
I0313 00:03:36.900795  114910 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0313 00:03:36.907037  114910 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0313 00:03:36.907890  114910 api_server.go:141] control plane version: v1.32.0
I0313 00:03:36.907915  114910 api_server.go:131] duration metric: took 7.133611ms to wait for apiserver health ...
I0313 00:03:36.907927  114910 system_pods.go:43] waiting for kube-system pods to appear ...
I0313 00:03:36.909039  114910 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I0313 00:03:36.910722  114910 addons.go:514] duration metric: took 662.835071ms for enable addons: enabled=[storage-provisioner default-storageclass]
I0313 00:03:36.914187  114910 system_pods.go:59] 5 kube-system pods found
I0313 00:03:36.914209  114910 system_pods.go:61] "etcd-minikube" [0e88e2d3-ccaa-4f95-a03d-2da2bd767eb3] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0313 00:03:36.914218  114910 system_pods.go:61] "kube-apiserver-minikube" [605c66e2-a3ca-4cba-a4c5-114c9ad0938f] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0313 00:03:36.914226  114910 system_pods.go:61] "kube-controller-manager-minikube" [ca9e6c7b-bc55-408f-a4bf-2e82eb000f82] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0313 00:03:36.914232  114910 system_pods.go:61] "kube-scheduler-minikube" [da12f48a-12d0-448b-81ff-410b8472a029] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0313 00:03:36.914235  114910 system_pods.go:61] "storage-provisioner" [7cb19be2-739c-4037-92d0-12d65af4b1a1] Pending
I0313 00:03:36.914241  114910 system_pods.go:74] duration metric: took 6.308262ms to wait for pod list to return data ...
I0313 00:03:36.914251  114910 kubeadm.go:582] duration metric: took 666.328756ms to wait for: map[apiserver:true system_pods:true]
I0313 00:03:36.914262  114910 node_conditions.go:102] verifying NodePressure condition ...
I0313 00:03:36.917290  114910 node_conditions.go:122] node storage ephemeral capacity is 153928Mi
I0313 00:03:36.917312  114910 node_conditions.go:123] node cpu capacity is 12
I0313 00:03:36.917323  114910 node_conditions.go:105] duration metric: took 3.05679ms to run NodePressure ...
I0313 00:03:36.917334  114910 start.go:241] waiting for startup goroutines ...
I0313 00:03:37.182909  114910 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0313 00:03:37.182950  114910 start.go:246] waiting for cluster config update ...
I0313 00:03:37.182969  114910 start.go:255] writing updated cluster config ...
I0313 00:03:37.183417  114910 ssh_runner.go:195] Run: rm -f paused
I0313 00:03:37.257187  114910 start.go:600] kubectl: 1.29.14, cluster: 1.32.0 (minor skew: 3)
I0313 00:03:37.258893  114910 out.go:201] 
W0313 00:03:37.260725  114910 out.go:270] ‚ùó  /usr/bin/kubectl is version 1.29.14, which may have incompatibilities with Kubernetes 1.32.0.
I0313 00:03:37.262314  114910 out.go:177]     ‚ñ™ Want kubectl v1.32.0? Try 'minikube kubectl -- get pods -A'
I0313 00:03:37.265636  114910 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Mar 12 18:33:12 minikube dockerd[1308]: time="2025-03-12T18:33:12.209770460Z" level=info msg="API listen on /var/run/docker.sock"
Mar 12 18:33:12 minikube dockerd[1308]: time="2025-03-12T18:33:12.209796986Z" level=info msg="API listen on [::]:2376"
Mar 12 18:33:12 minikube systemd[1]: Started Docker Application Container Engine.
Mar 12 18:33:12 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Mar 12 18:33:12 minikube cri-dockerd[1594]: time="2025-03-12T18:33:12Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Mar 12 18:33:12 minikube cri-dockerd[1594]: time="2025-03-12T18:33:12Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Mar 12 18:33:12 minikube cri-dockerd[1594]: time="2025-03-12T18:33:12Z" level=info msg="Start docker client with request timeout 0s"
Mar 12 18:33:12 minikube cri-dockerd[1594]: time="2025-03-12T18:33:12Z" level=info msg="Hairpin mode is set to hairpin-veth"
Mar 12 18:33:12 minikube cri-dockerd[1594]: time="2025-03-12T18:33:12Z" level=info msg="Loaded network plugin cni"
Mar 12 18:33:12 minikube cri-dockerd[1594]: time="2025-03-12T18:33:12Z" level=info msg="Docker cri networking managed by network plugin cni"
Mar 12 18:33:12 minikube cri-dockerd[1594]: time="2025-03-12T18:33:12Z" level=info msg="Setting cgroupDriver systemd"
Mar 12 18:33:12 minikube cri-dockerd[1594]: time="2025-03-12T18:33:12Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Mar 12 18:33:12 minikube cri-dockerd[1594]: time="2025-03-12T18:33:12Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Mar 12 18:33:12 minikube cri-dockerd[1594]: time="2025-03-12T18:33:12Z" level=info msg="Start cri-dockerd grpc backend"
Mar 12 18:33:12 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Mar 12 18:33:20 minikube systemd[1]: Stopping Docker Application Container Engine...
Mar 12 18:33:20 minikube dockerd[1308]: time="2025-03-12T18:33:20.909884033Z" level=info msg="Processing signal 'terminated'"
Mar 12 18:33:20 minikube dockerd[1308]: time="2025-03-12T18:33:20.912578337Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Mar 12 18:33:20 minikube dockerd[1308]: time="2025-03-12T18:33:20.913134738Z" level=info msg="Daemon shutdown complete"
Mar 12 18:33:20 minikube dockerd[1308]: time="2025-03-12T18:33:20.913190513Z" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=plugins.moby
Mar 12 18:33:20 minikube systemd[1]: docker.service: Deactivated successfully.
Mar 12 18:33:20 minikube systemd[1]: Stopped Docker Application Container Engine.
Mar 12 18:33:20 minikube systemd[1]: Starting Docker Application Container Engine...
Mar 12 18:33:21 minikube dockerd[1760]: time="2025-03-12T18:33:21.099886782Z" level=info msg="Starting up"
Mar 12 18:33:21 minikube dockerd[1760]: time="2025-03-12T18:33:21.101634482Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Mar 12 18:33:21 minikube dockerd[1760]: time="2025-03-12T18:33:21.280260681Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Mar 12 18:33:22 minikube dockerd[1760]: time="2025-03-12T18:33:22.597126276Z" level=info msg="Loading containers: start."
Mar 12 18:33:23 minikube dockerd[1760]: time="2025-03-12T18:33:23.324004962Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Mar 12 18:33:23 minikube dockerd[1760]: time="2025-03-12T18:33:23.535686312Z" level=info msg="Loading containers: done."
Mar 12 18:33:23 minikube dockerd[1760]: time="2025-03-12T18:33:23.669068088Z" level=info msg="Docker daemon" commit=c710b88 containerd-snapshotter=false storage-driver=overlay2 version=27.4.1
Mar 12 18:33:23 minikube dockerd[1760]: time="2025-03-12T18:33:23.669169340Z" level=info msg="Daemon has completed initialization"
Mar 12 18:33:23 minikube dockerd[1760]: time="2025-03-12T18:33:23.719601616Z" level=info msg="API listen on /var/run/docker.sock"
Mar 12 18:33:23 minikube dockerd[1760]: time="2025-03-12T18:33:23.719609763Z" level=info msg="API listen on [::]:2376"
Mar 12 18:33:23 minikube systemd[1]: Started Docker Application Container Engine.
Mar 12 18:33:29 minikube cri-dockerd[1594]: time="2025-03-12T18:33:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f259b22ae583029b2ba05d354e4f1f6e8f3beef599eea0eddff696980b70e463/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Mar 12 18:33:29 minikube cri-dockerd[1594]: time="2025-03-12T18:33:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/446ec0d11148b20df7f73680515f35c498ef451807750c088f55e4712e97075e/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Mar 12 18:33:29 minikube cri-dockerd[1594]: time="2025-03-12T18:33:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/154c789a0cec8cc245a9374b029a0183e642f293b205a03318a35ebefd3a3f12/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Mar 12 18:33:29 minikube cri-dockerd[1594]: time="2025-03-12T18:33:29Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6080390b52133c3549ecf23c9c9667c6e51a1e2e0c6144e804bc0ae7d2b70c4f/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Mar 12 18:33:40 minikube cri-dockerd[1594]: time="2025-03-12T18:33:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8257578645f0485a874d90d327f181e2d20d95130d1971d536196e5505fede67/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Mar 12 18:33:40 minikube cri-dockerd[1594]: time="2025-03-12T18:33:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/098b4ee6963454096075fe3fa4d1e7017a8c52ee5a09051a2f03450c2ac629ed/resolv.conf as [nameserver 192.168.49.1 options ndots:0 edns0 trust-ad]"
Mar 12 18:33:41 minikube cri-dockerd[1594]: time="2025-03-12T18:33:41Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0f9620a37989f20f5e605c9d133b4cc51d8c12e300bacd066a6b3847440645ee/resolv.conf as [nameserver 192.168.49.1 options edns0 trust-ad ndots:0]"
Mar 12 18:33:45 minikube cri-dockerd[1594]: time="2025-03-12T18:33:45Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Mar 12 18:34:11 minikube dockerd[1760]: time="2025-03-12T18:34:11.215385449Z" level=info msg="ignoring event" container=c2e08d44fdf05d028757cd894db730301b668327dad913f7158b118cc84c3091 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Mar 12 18:34:35 minikube cri-dockerd[1594]: time="2025-03-12T18:34:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c1124f80d362785e8a66604881227f669461be6740ef85353369a1c9c2a05ff9/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 12 18:34:39 minikube dockerd[1760]: time="2025-03-12T18:34:39.068163171Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Mar 12 18:34:39 minikube dockerd[1760]: time="2025-03-12T18:34:39.068260471Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Mar 12 18:34:56 minikube dockerd[1760]: time="2025-03-12T18:34:56.646382347Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Mar 12 18:34:56 minikube dockerd[1760]: time="2025-03-12T18:34:56.646500309Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Mar 12 18:35:24 minikube dockerd[1760]: time="2025-03-12T18:35:24.589196859Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Mar 12 18:35:24 minikube dockerd[1760]: time="2025-03-12T18:35:24.589295007Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Mar 12 18:36:10 minikube dockerd[1760]: time="2025-03-12T18:36:10.751224785Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Mar 12 18:36:10 minikube dockerd[1760]: time="2025-03-12T18:36:10.751315089Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Mar 12 18:37:40 minikube dockerd[1760]: time="2025-03-12T18:37:40.563076710Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Mar 12 18:37:40 minikube dockerd[1760]: time="2025-03-12T18:37:40.563123789Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Mar 12 18:40:35 minikube dockerd[1760]: time="2025-03-12T18:40:35.834433645Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Mar 12 18:40:35 minikube dockerd[1760]: time="2025-03-12T18:40:35.834501588Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Mar 12 18:45:49 minikube dockerd[1760]: time="2025-03-12T18:45:49.666338199Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Mar 12 18:45:49 minikube dockerd[1760]: time="2025-03-12T18:45:49.666918203Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Mar 12 18:51:06 minikube dockerd[1760]: time="2025-03-12T18:51:06.677745364Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Mar 12 18:51:06 minikube dockerd[1760]: time="2025-03-12T18:51:06.677804228Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
d8c35f7e85d7f       6e38f40d628db       18 minutes ago      Running             storage-provisioner       1                   0f9620a37989f       storage-provisioner
c2e08d44fdf05       6e38f40d628db       18 minutes ago      Exited              storage-provisioner       0                   0f9620a37989f       storage-provisioner
2fa312d503110       040f9f8aac8cd       18 minutes ago      Running             kube-proxy                0                   098b4ee696345       kube-proxy-7tgj5
c1d53abb58407       c69fa2e9cbf5f       18 minutes ago      Running             coredns                   0                   8257578645f04       coredns-668d6bf9bc-pvq8q
097bf8efc1aae       a389e107f4ff1       19 minutes ago      Running             kube-scheduler            0                   f259b22ae5830       kube-scheduler-minikube
a22f49a42cdec       a9e7e6b294baf       19 minutes ago      Running             etcd                      0                   154c789a0cec8       etcd-minikube
286dc388719e0       c2e17b8d0f4a3       19 minutes ago      Running             kube-apiserver            0                   446ec0d11148b       kube-apiserver-minikube
f821de59c4e5a       8cab3d2a8bd0f       19 minutes ago      Running             kube-controller-manager   0                   6080390b52133       kube-controller-manager-minikube


==> coredns [c1d53abb5840] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:47727 - 56584 "HINFO IN 4799711540926232970.4595154869985579648. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.062780551s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1503317202]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (12-Mar-2025 18:33:40.963) (total time: 30001ms):
Trace[1503317202]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30001ms (18:34:10.964)
Trace[1503317202]: [30.001728071s] [30.001728071s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[412455052]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (12-Mar-2025 18:33:40.963) (total time: 30001ms):
Trace[412455052]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30001ms (18:34:10.964)
Trace[412455052]: [30.001874744s] [30.001874744s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[129224146]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (12-Mar-2025 18:33:40.963) (total time: 30002ms):
Trace[129224146]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30001ms (18:34:10.964)
Trace[129224146]: [30.002626058s] [30.002626058s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_03_13T00_03_36_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 12 Mar 2025 18:33:32 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 12 Mar 2025 18:52:28 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 12 Mar 2025 18:48:43 +0000   Wed, 12 Mar 2025 18:33:31 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 12 Mar 2025 18:48:43 +0000   Wed, 12 Mar 2025 18:33:31 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 12 Mar 2025 18:48:43 +0000   Wed, 12 Mar 2025 18:33:31 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 12 Mar 2025 18:48:43 +0000   Wed, 12 Mar 2025 18:33:32 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  153928Mi
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7808144Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  153928Mi
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7808144Ki
  pods:               110
System Info:
  Machine ID:                 41a93d9b4ac8442b8ac4f0c8173e16bc
  System UUID:                fedc04a3-fb2e-4f4e-811a-ee34d62cf60b
  Boot ID:                    2faa8e3d-5e04-44d9-9274-8602e03114ca
  Kernel Version:             6.13.5-200.fc41.x86_64
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                 CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                 ------------  ----------  ---------------  -------------  ---
  default                     go-microservice1-765c877587-s57j8    0 (0%)        0 (0%)      0 (0%)           0 (0%)         18m
  kube-system                 coredns-668d6bf9bc-pvq8q             100m (0%)     0 (0%)      70Mi (0%)        170Mi (2%)     18m
  kube-system                 etcd-minikube                        100m (0%)     0 (0%)      100Mi (1%)       0 (0%)         19m
  kube-system                 kube-apiserver-minikube              250m (2%)     0 (0%)      0 (0%)           0 (0%)         19m
  kube-system                 kube-controller-manager-minikube     200m (1%)     0 (0%)      0 (0%)           0 (0%)         18m
  kube-system                 kube-proxy-7tgj5                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         18m
  kube-system                 kube-scheduler-minikube              100m (0%)     0 (0%)      0 (0%)           0 (0%)         19m
  kube-system                 storage-provisioner                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         18m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (6%)   0 (0%)
  memory             170Mi (2%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 18m                kube-proxy       
  Normal  Starting                 19m                kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  19m (x8 over 19m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    19m (x8 over 19m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     19m (x7 over 19m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  19m                kubelet          Updated Node Allocatable limit across pods
  Normal  Starting                 18m                kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  18m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  18m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    18m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     18m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           18m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +0.000418] pci 10000:e0:06.0: Primary bus is hard wired to 0
[  +0.099450] pxa2xx-spi pxa2xx-spi.3: no DMA channels available, using PIO
[  +0.121122] pcieport 10000:e0:06.0: can't derive routing for PCI INT D
[  +0.000001] pcieport 10000:e0:06.0: PCI INT D: no GSI
[  +0.180774] pcieport 10000:e0:06.0: can't derive routing for PCI INT A
[  +0.000002] nvme 10000:e1:00.0: PCI INT A: not connected
[  +0.045102] tpm tpm0: auth session is active
[  +1.747552] ACPI BIOS Error (bug): Could not resolve symbol [\CTDP], AE_NOT_FOUND (20240827/psargs-332)
[  +0.000019] ACPI Error: Aborting method \_SB.IETM.IDSP due to previous error (AE_NOT_FOUND) (20240827/psparse-529)
[  +0.000033] ACPI Warning: \_SB.IETM._TRT: Return Package has no elements (empty) (20240827/nsprepkg-94)
[  +0.008217] ACPI BIOS Error (bug): Could not resolve symbol [\_SB.PC00.LPCB.EC0.SEN1._CRT.S1CT], AE_NOT_FOUND (20240827/psargs-332)
[  +0.000010] ACPI Error: Aborting method \_SB.PC00.LPCB.EC0.SEN1._CRT due to previous error (AE_NOT_FOUND) (20240827/psparse-529)
[  +0.000028] ACPI BIOS Error (bug): Could not resolve symbol [\_SB.PC00.LPCB.EC0.SEN1._HOT.S1HT], AE_NOT_FOUND (20240827/psargs-332)
[  +0.000004] ACPI Error: Aborting method \_SB.PC00.LPCB.EC0.SEN1._HOT due to previous error (AE_NOT_FOUND) (20240827/psparse-529)
[  +0.000029] ACPI BIOS Error (bug): Could not resolve symbol [\_SB.PC00.LPCB.EC0.SEN1._PSV.S1PT], AE_NOT_FOUND (20240827/psargs-332)
[  +0.000003] ACPI Error: Aborting method \_SB.PC00.LPCB.EC0.SEN1._PSV due to previous error (AE_NOT_FOUND) (20240827/psparse-529)
[  +0.000027] ACPI BIOS Error (bug): Could not resolve symbol [\_SB.PC00.LPCB.EC0.SEN1._AC0.S1AT], AE_NOT_FOUND (20240827/psargs-332)
[  +0.000003] ACPI Error: Aborting method \_SB.PC00.LPCB.EC0.SEN1._AC0 due to previous error (AE_NOT_FOUND) (20240827/psparse-529)
[  +0.019918] ACPI BIOS Error (bug): Could not resolve symbol [\_SB.PC00.LPCB.EC0.SEN2._CRT.S2CT], AE_NOT_FOUND (20240827/psargs-332)
[  +0.000008] ACPI Error: Aborting method \_SB.PC00.LPCB.EC0.SEN2._CRT due to previous error (AE_NOT_FOUND) (20240827/psparse-529)
[  +0.000024] ACPI BIOS Error (bug): Could not resolve symbol [\_SB.PC00.LPCB.EC0.SEN2._HOT.S2HT], AE_NOT_FOUND (20240827/psargs-332)
[  +0.000003] ACPI Error: Aborting method \_SB.PC00.LPCB.EC0.SEN2._HOT due to previous error (AE_NOT_FOUND) (20240827/psparse-529)
[  +0.000023] ACPI BIOS Error (bug): Could not resolve symbol [\_SB.PC00.LPCB.EC0.SEN2._PSV.S2PT], AE_NOT_FOUND (20240827/psargs-332)
[  +0.000004] ACPI Error: Aborting method \_SB.PC00.LPCB.EC0.SEN2._PSV due to previous error (AE_NOT_FOUND) (20240827/psparse-529)
[  +0.000026] ACPI BIOS Error (bug): Could not resolve symbol [\_SB.PC00.LPCB.EC0.SEN2._AC0.S2AT], AE_NOT_FOUND (20240827/psargs-332)
[  +0.000002] ACPI Error: Aborting method \_SB.PC00.LPCB.EC0.SEN2._AC0 due to previous error (AE_NOT_FOUND) (20240827/psparse-529)
[  +0.028638] ACPI BIOS Error (bug): Could not resolve symbol [\_SB.PC00.LPCB.EC0.SEN3._CRT.S3CT], AE_NOT_FOUND (20240827/psargs-332)
[  +0.000023] ACPI Error: Aborting method \_SB.PC00.LPCB.EC0.SEN3._CRT due to previous error (AE_NOT_FOUND) (20240827/psparse-529)
[  +0.000228] ACPI BIOS Error (bug): Could not resolve symbol [\_SB.PC00.LPCB.EC0.SEN3._HOT.S3HT], AE_NOT_FOUND (20240827/psargs-332)
[  +0.000018] ACPI Error: Aborting method \_SB.PC00.LPCB.EC0.SEN3._HOT due to previous error (AE_NOT_FOUND)
[  +0.000001] resource: resource sanity check: requesting [mem 0x00000000fedc0000-0x00000000fedcffff], which spans more than pnp 00:03 [mem 0xfedc0000-0xfedc7fff]
[  +0.000005]  (20240827/psparse-529)
[  +0.000002] caller igen6_probe+0x30a/0x825 [igen6_edac] mapping multiple BARs
[  +0.000037] ACPI BIOS Error (bug): Could not resolve symbol [\_SB.PC00.LPCB.EC0.SEN3._PSV.S3PT], AE_NOT_FOUND (20240827/psargs-332)
[  +0.000005] ACPI Error: Aborting method \_SB.PC00.LPCB.EC0.SEN3._PSV due to previous error (AE_NOT_FOUND) (20240827/psparse-529)
[  +0.016079] ACPI BIOS Error (bug): Could not resolve symbol [\_SB.PC00.LPCB.EC0.SEN4._CRT.S4CT], AE_NOT_FOUND (20240827/psargs-332)
[  +0.000010] ACPI Error: Aborting method \_SB.PC00.LPCB.EC0.SEN4._CRT due to previous error (AE_NOT_FOUND) (20240827/psparse-529)
[  +0.000024] ACPI BIOS Error (bug): Could not resolve symbol [\_SB.PC00.LPCB.EC0.SEN4._HOT.S4HT], AE_NOT_FOUND (20240827/psargs-332)
[  +0.000002] ACPI Error: Aborting method \_SB.PC00.LPCB.EC0.SEN4._HOT due to previous error (AE_NOT_FOUND) (20240827/psparse-529)
[  +0.000023] ACPI BIOS Error (bug): Could not resolve symbol [\_SB.PC00.LPCB.EC0.SEN4._PSV.S4PT], AE_NOT_FOUND (20240827/psargs-332)
[  +0.000002] ACPI Error: Aborting method \_SB.PC00.LPCB.EC0.SEN4._PSV due to previous error (AE_NOT_FOUND) (20240827/psparse-529)
[  +0.000022] ACPI BIOS Error (bug): Could not resolve symbol [\_SB.PC00.LPCB.EC0.SEN4._AC0.S4AT], AE_NOT_FOUND (20240827/psargs-332)
[  +0.000003] ACPI Error: Aborting method \_SB.PC00.LPCB.EC0.SEN4._AC0 due to previous error (AE_NOT_FOUND) (20240827/psparse-529)
[  +0.133660] asus_wmi: fan_curve_get_factory_default (0x00110024) failed: -19
[  +0.003347] asus_wmi: fan_curve_get_factory_default (0x00110025) failed: -19
[  +0.000573] asus_wmi: fan_curve_get_factory_default (0x00110032) failed: -19
[  +0.262785] mt7921e 0000:02:00.0: sar cnt = 0
[  +1.300042] nvme nvme0: using unchecked data buffer
[  +0.144530] block nvme0n1: No UUID available providing old NGUID
[  +1.524841] Bluetooth: hci0: HCI Enhanced Setup Synchronous Connection command is advertised, but not supported.
[Mar12 14:59] queueing ieee80211 work while going to suspend
[Mar12 15:20] done.
[Mar12 15:30] lockdown_is_locked_down: 9 callbacks suppressed
[Mar12 15:31] Bluetooth: hci0: HCI Enhanced Setup Synchronous Connection command is advertised, but not supported.
[ +11.462950] hid-generic 0005:05D6:000A.0002: unknown main item tag 0x0
[Mar12 16:21] queueing ieee80211 work while going to suspend
[Mar12 18:03] done.
[  +8.452592] lockdown_is_locked_down: 8 callbacks suppressed
[Mar12 18:04] Bluetooth: hci0: ACL packet for unknown connection handle 50
[ +21.063294] hid-generic 0005:05D6:000A.0003: unknown main item tag 0x0


==> etcd [a22f49a42cde] <==
{"level":"warn","ts":"2025-03-12T18:33:30.534154Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-03-12T18:33:30.534348Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"warn","ts":"2025-03-12T18:33:30.534516Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-03-12T18:33:30.534534Z","caller":"embed/etcd.go:128","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-03-12T18:33:30.534596Z","caller":"embed/etcd.go:497","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-03-12T18:33:30.535580Z","caller":"embed/etcd.go:136","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2025-03-12T18:33:30.535978Z","caller":"embed/etcd.go:311","msg":"starting an etcd server","etcd-version":"3.5.16","git-sha":"f20bbad","go-version":"go1.22.7","go-os":"linux","go-arch":"amd64","max-cpu-set":12,"max-cpu-available":12,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://192.168.49.2:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2025-03-12T18:33:30.539059Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"2.513894ms"}
{"level":"info","ts":"2025-03-12T18:33:30.545387Z","caller":"etcdserver/raft.go:505","msg":"starting local member","local-member-id":"aec36adc501070cc","cluster-id":"fa54960ea34d58be"}
{"level":"info","ts":"2025-03-12T18:33:30.545532Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2025-03-12T18:33:30.545600Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 0"}
{"level":"info","ts":"2025-03-12T18:33:30.545623Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2025-03-12T18:33:30.545643Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 1"}
{"level":"info","ts":"2025-03-12T18:33:30.545722Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"warn","ts":"2025-03-12T18:33:30.551583Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-03-12T18:33:30.557446Z","caller":"mvcc/kvstore.go:423","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2025-03-12T18:33:30.563044Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-03-12T18:33:30.568083Z","caller":"etcdserver/server.go:873","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.16","cluster-version":"to_be_decided"}
{"level":"info","ts":"2025-03-12T18:33:30.568405Z","caller":"etcdserver/server.go:757","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2025-03-12T18:33:30.568517Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-03-12T18:33:30.568710Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-03-12T18:33:30.568777Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-03-12T18:33:30.569845Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-03-12T18:33:30.571305Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-03-12T18:33:30.571495Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-03-12T18:33:30.573828Z","caller":"embed/etcd.go:729","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-03-12T18:33:30.574042Z","caller":"embed/etcd.go:600","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-03-12T18:33:30.574108Z","caller":"embed/etcd.go:572","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-03-12T18:33:30.574669Z","caller":"embed/etcd.go:280","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-03-12T18:33:30.574761Z","caller":"embed/etcd.go:871","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-03-12T18:33:31.446633Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2025-03-12T18:33:31.446690Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2025-03-12T18:33:31.446722Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2025-03-12T18:33:31.446744Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2025-03-12T18:33:31.446753Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-03-12T18:33:31.446764Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2025-03-12T18:33:31.446774Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-03-12T18:33:31.450420Z","caller":"etcdserver/server.go:2651","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2025-03-12T18:33:31.452128Z","caller":"etcdserver/server.go:2140","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-03-12T18:33:31.452152Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-03-12T18:33:31.452167Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-03-12T18:33:31.452427Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-03-12T18:33:31.452495Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-03-12T18:33:31.453096Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-03-12T18:33:31.453102Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-03-12T18:33:31.453749Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2025-03-12T18:33:31.454147Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-03-12T18:33:31.454227Z","caller":"etcdserver/server.go:2675","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2025-03-12T18:33:31.454608Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-03-12T18:33:31.454761Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-03-12T18:43:31.482687Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":706}
{"level":"info","ts":"2025-03-12T18:43:31.502961Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":706,"took":"19.660631ms","hash":2906111619,"current-db-size-bytes":1716224,"current-db-size":"1.7 MB","current-db-size-in-use-bytes":1716224,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-03-12T18:43:31.503015Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2906111619,"revision":706,"compact-revision":-1}
{"level":"info","ts":"2025-03-12T18:48:31.497903Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":959}
{"level":"info","ts":"2025-03-12T18:48:31.521376Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":959,"took":"22.686071ms","hash":2993407809,"current-db-size-bytes":1716224,"current-db-size":"1.7 MB","current-db-size-in-use-bytes":1110016,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2025-03-12T18:48:31.521475Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2993407809,"revision":959,"compact-revision":706}


==> kernel <==
 18:52:34 up  6:00,  0 users,  load average: 0.45, 0.88, 0.89
Linux minikube 6.13.5-200.fc41.x86_64 #1 SMP PREEMPT_DYNAMIC Thu Feb 27 15:07:31 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [286dc388719e] <==
I0312 18:33:32.342413       1 aggregator.go:169] waiting for initial CRD sync...
I0312 18:33:32.342442       1 local_available_controller.go:156] Starting LocalAvailability controller
I0312 18:33:32.342522       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0312 18:33:32.342538       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0312 18:33:32.342486       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0312 18:33:32.342655       1 controller.go:119] Starting legacy_token_tracking_controller
I0312 18:33:32.342668       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0312 18:33:32.342795       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0312 18:33:32.342808       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0312 18:33:32.353339       1 cluster_authentication_trust_controller.go:462] Starting cluster_authentication_trust_controller controller
I0312 18:33:32.342458       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0312 18:33:32.342491       1 controller.go:78] Starting OpenAPI AggregationController
I0312 18:33:32.353454       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0312 18:33:32.353581       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0312 18:33:32.353604       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0312 18:33:32.353628       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0312 18:33:32.353661       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0312 18:33:32.353764       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0312 18:33:32.353788       1 naming_controller.go:294] Starting NamingConditionController
I0312 18:33:32.353829       1 establishing_controller.go:81] Starting EstablishingController
I0312 18:33:32.353887       1 controller.go:142] Starting OpenAPI controller
I0312 18:33:32.353907       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0312 18:33:32.353768       1 controller.go:90] Starting OpenAPI V3 controller
I0312 18:33:32.353892       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0312 18:33:32.353953       1 crd_finalizer.go:269] Starting CRDFinalizer
I0312 18:33:32.421391       1 shared_informer.go:320] Caches are synced for node_authorizer
I0312 18:33:32.431930       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0312 18:33:32.431968       1 policy_source.go:240] refreshing policies
I0312 18:33:32.443288       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0312 18:33:32.443376       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0312 18:33:32.443558       1 shared_informer.go:320] Caches are synced for configmaps
I0312 18:33:32.443613       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0312 18:33:32.443697       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0312 18:33:32.443718       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0312 18:33:32.443827       1 cache.go:39] Caches are synced for LocalAvailability controller
I0312 18:33:32.446277       1 controller.go:615] quota admission added evaluator for: namespaces
E0312 18:33:32.451744       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I0312 18:33:32.453694       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0312 18:33:32.453742       1 aggregator.go:171] initial CRD sync complete...
I0312 18:33:32.453756       1 autoregister_controller.go:144] Starting autoregister controller
I0312 18:33:32.453769       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0312 18:33:32.453779       1 cache.go:39] Caches are synced for autoregister controller
I0312 18:33:32.453909       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0312 18:33:32.658393       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0312 18:33:33.354811       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0312 18:33:33.362614       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0312 18:33:33.362644       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0312 18:33:34.172310       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0312 18:33:34.226707       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0312 18:33:34.359320       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0312 18:33:34.374620       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0312 18:33:34.376455       1 controller.go:615] quota admission added evaluator for: endpoints
I0312 18:33:34.383977       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0312 18:33:34.389559       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0312 18:33:35.429147       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0312 18:33:35.443910       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0312 18:33:35.456314       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0312 18:33:39.741993       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I0312 18:33:39.894548       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0312 18:36:08.580811       1 alloc.go:330] "allocated clusterIPs" service="default/go-microservice1" clusterIPs={"IPv4":"10.105.255.137"}


==> kube-controller-manager [f821de59c4e5] <==
I0312 18:33:38.938544       1 node_lifecycle_controller.go:1234] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0312 18:33:38.938692       1 node_lifecycle_controller.go:886] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0312 18:33:38.938768       1 node_lifecycle_controller.go:1080] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0312 18:33:38.940116       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0312 18:33:38.940200       1 shared_informer.go:320] Caches are synced for expand
I0312 18:33:38.940513       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0312 18:33:38.940627       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0312 18:33:38.940914       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0312 18:33:38.940940       1 shared_informer.go:320] Caches are synced for stateful set
I0312 18:33:38.941060       1 shared_informer.go:320] Caches are synced for TTL after finished
I0312 18:33:38.941460       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0312 18:33:38.942326       1 shared_informer.go:320] Caches are synced for TTL
I0312 18:33:38.942367       1 shared_informer.go:320] Caches are synced for namespace
I0312 18:33:38.942409       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0312 18:33:38.942541       1 shared_informer.go:320] Caches are synced for ephemeral
I0312 18:33:38.944956       1 shared_informer.go:320] Caches are synced for deployment
I0312 18:33:38.945751       1 shared_informer.go:320] Caches are synced for node
I0312 18:33:38.945894       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0312 18:33:38.946098       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0312 18:33:38.946122       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0312 18:33:38.946135       1 shared_informer.go:320] Caches are synced for cidrallocator
I0312 18:33:38.951089       1 shared_informer.go:320] Caches are synced for resource quota
I0312 18:33:38.952132       1 shared_informer.go:320] Caches are synced for PV protection
I0312 18:33:38.961094       1 shared_informer.go:320] Caches are synced for crt configmap
I0312 18:33:38.966712       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0312 18:33:38.966772       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0312 18:33:38.966816       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0312 18:33:38.969334       1 shared_informer.go:320] Caches are synced for garbage collector
I0312 18:33:39.848483       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0312 18:33:40.053691       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="141.175048ms"
I0312 18:33:40.065973       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="12.023964ms"
I0312 18:33:40.066180       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="124.491¬µs"
I0312 18:33:40.077920       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="238.926¬µs"
I0312 18:33:41.452255       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="111.145¬µs"
I0312 18:33:45.645719       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0312 18:34:18.382371       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="10.840428ms"
I0312 18:34:18.382570       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="113.701¬µs"
I0312 18:34:34.913909       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/go-microservice1-765c877587" duration="21.771543ms"
I0312 18:34:34.925780       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/go-microservice1-765c877587" duration="11.765028ms"
I0312 18:34:34.926068       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/go-microservice1-765c877587" duration="131.982¬µs"
I0312 18:34:34.932818       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/go-microservice1-765c877587" duration="97.923¬µs"
I0312 18:34:39.953940       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/go-microservice1-765c877587" duration="183.837¬µs"
I0312 18:34:53.324410       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/go-microservice1-765c877587" duration="360.275¬µs"
I0312 18:35:09.320265       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/go-microservice1-765c877587" duration="111.679¬µs"
I0312 18:35:21.318890       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/go-microservice1-765c877587" duration="105.282¬µs"
I0312 18:35:39.315919       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/go-microservice1-765c877587" duration="115.59¬µs"
I0312 18:35:52.321751       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/go-microservice1-765c877587" duration="184.776¬µs"
I0312 18:36:22.315359       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/go-microservice1-765c877587" duration="94.441¬µs"
I0312 18:36:34.313953       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/go-microservice1-765c877587" duration="121.161¬µs"
I0312 18:37:55.315821       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/go-microservice1-765c877587" duration="114.379¬µs"
I0312 18:38:10.312622       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/go-microservice1-765c877587" duration="80.857¬µs"
I0312 18:38:50.568835       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0312 18:40:50.316336       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/go-microservice1-765c877587" duration="78.919¬µs"
I0312 18:41:02.317400       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/go-microservice1-765c877587" duration="77.56¬µs"
I0312 18:43:56.211203       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0312 18:46:00.323704       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/go-microservice1-765c877587" duration="144.897¬µs"
I0312 18:46:15.327691       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/go-microservice1-765c877587" duration="410.406¬µs"
I0312 18:48:43.111806       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0312 18:51:20.318797       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/go-microservice1-765c877587" duration="143.721¬µs"
I0312 18:51:31.321865       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/go-microservice1-765c877587" duration="268.155¬µs"


==> kube-proxy [2fa312d50311] <==
I0312 18:33:41.231424       1 server_linux.go:66] "Using iptables proxy"
I0312 18:33:41.344393       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0312 18:33:41.344471       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0312 18:33:41.372007       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0312 18:33:41.372075       1 server_linux.go:170] "Using iptables Proxier"
I0312 18:33:41.374926       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0312 18:33:41.382698       1 server.go:497] "Version info" version="v1.32.0"
I0312 18:33:41.382734       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0312 18:33:41.384828       1 config.go:105] "Starting endpoint slice config controller"
I0312 18:33:41.384935       1 config.go:199] "Starting service config controller"
I0312 18:33:41.385046       1 config.go:329] "Starting node config controller"
I0312 18:33:41.384987       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0312 18:33:41.385057       1 shared_informer.go:313] Waiting for caches to sync for node config
I0312 18:33:41.385034       1 shared_informer.go:313] Waiting for caches to sync for service config
I0312 18:33:41.486013       1 shared_informer.go:320] Caches are synced for service config
I0312 18:33:41.486066       1 shared_informer.go:320] Caches are synced for node config
I0312 18:33:41.486016       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-scheduler [097bf8efc1aa] <==
E0312 18:33:32.421323       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0312 18:33:32.421364       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0312 18:33:32.421364       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0312 18:33:32.421389       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0312 18:33:32.421423       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
E0312 18:33:32.421423       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0312 18:33:32.421426       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0312 18:33:32.422234       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0312 18:33:32.421496       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0312 18:33:32.422316       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0312 18:33:32.422284       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0312 18:33:32.421473       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0312 18:33:32.422412       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0312 18:33:32.423646       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0312 18:33:32.423750       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0312 18:33:32.423749       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0312 18:33:32.423799       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0312 18:33:32.423809       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0312 18:33:32.423818       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0312 18:33:32.423874       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0312 18:33:32.423879       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0312 18:33:32.423882       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0312 18:33:32.423919       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E0312 18:33:32.423805       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E0312 18:33:32.423983       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0312 18:33:32.424046       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0312 18:33:32.424103       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0312 18:33:32.424150       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0312 18:33:32.424181       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0312 18:33:32.424928       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0312 18:33:32.424977       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0312 18:33:33.279126       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0312 18:33:33.279212       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0312 18:33:33.361326       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0312 18:33:33.361411       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0312 18:33:33.394406       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0312 18:33:33.394474       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0312 18:33:33.404024       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0312 18:33:33.404086       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0312 18:33:33.565070       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0312 18:33:33.565138       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0312 18:33:33.602113       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0312 18:33:33.602183       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0312 18:33:33.708381       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0312 18:33:33.708448       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0312 18:33:33.734685       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0312 18:33:33.734765       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0312 18:33:33.741044       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0312 18:33:33.741105       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0312 18:33:33.743764       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0312 18:33:33.743834       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0312 18:33:33.761471       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0312 18:33:33.761560       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0312 18:33:33.784830       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0312 18:33:33.784927       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0312 18:33:33.786193       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0312 18:33:33.786254       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0312 18:33:33.894629       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0312 18:33:33.894712       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
I0312 18:33:35.619475       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Mar 12 18:41:02 minikube kubelet[2765]: E0312 18:41:02.301984    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:41:14 minikube kubelet[2765]: E0312 18:41:14.301337    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:41:28 minikube kubelet[2765]: E0312 18:41:28.301285    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:41:39 minikube kubelet[2765]: E0312 18:41:39.311090    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:41:50 minikube kubelet[2765]: E0312 18:41:50.301714    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:42:04 minikube kubelet[2765]: E0312 18:42:04.301303    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:42:19 minikube kubelet[2765]: E0312 18:42:19.301714    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:42:31 minikube kubelet[2765]: E0312 18:42:31.310822    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:42:42 minikube kubelet[2765]: E0312 18:42:42.303187    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:42:57 minikube kubelet[2765]: E0312 18:42:57.302264    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:43:10 minikube kubelet[2765]: E0312 18:43:10.301133    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:43:21 minikube kubelet[2765]: E0312 18:43:21.301650    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:43:36 minikube kubelet[2765]: E0312 18:43:36.302098    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:43:49 minikube kubelet[2765]: E0312 18:43:49.301628    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:44:03 minikube kubelet[2765]: E0312 18:44:03.301344    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:44:15 minikube kubelet[2765]: E0312 18:44:15.301721    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:44:27 minikube kubelet[2765]: E0312 18:44:27.301694    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:44:41 minikube kubelet[2765]: E0312 18:44:41.301679    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:44:55 minikube kubelet[2765]: E0312 18:44:55.301151    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:45:06 minikube kubelet[2765]: E0312 18:45:06.302546    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:45:19 minikube kubelet[2765]: E0312 18:45:19.301687    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:45:31 minikube kubelet[2765]: E0312 18:45:31.301369    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:45:49 minikube kubelet[2765]: E0312 18:45:49.676601    2765 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="go-microservice1:latest"
Mar 12 18:45:49 minikube kubelet[2765]: E0312 18:45:49.676752    2765 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="go-microservice1:latest"
Mar 12 18:45:49 minikube kubelet[2765]: E0312 18:45:49.677046    2765 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:go-microservice1,Image:go-microservice1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-znzqb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod go-microservice1-765c877587-s57j8_default(940c035a-14fa-4910-9a8f-a37bf8753cda): ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Mar 12 18:45:49 minikube kubelet[2765]: E0312 18:45:49.678295    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ErrImagePull: \"Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:46:00 minikube kubelet[2765]: E0312 18:46:00.302651    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:46:15 minikube kubelet[2765]: E0312 18:46:15.303629    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:46:27 minikube kubelet[2765]: E0312 18:46:27.311185    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:46:38 minikube kubelet[2765]: E0312 18:46:38.301002    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:46:50 minikube kubelet[2765]: E0312 18:46:50.301787    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:47:02 minikube kubelet[2765]: E0312 18:47:02.301874    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:47:13 minikube kubelet[2765]: E0312 18:47:13.300750    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:47:26 minikube kubelet[2765]: E0312 18:47:26.302076    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:47:39 minikube kubelet[2765]: E0312 18:47:39.302123    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:47:52 minikube kubelet[2765]: E0312 18:47:52.302385    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:48:03 minikube kubelet[2765]: E0312 18:48:03.301314    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:48:16 minikube kubelet[2765]: E0312 18:48:16.301429    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:48:28 minikube kubelet[2765]: E0312 18:48:28.301524    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:48:40 minikube kubelet[2765]: E0312 18:48:40.301909    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:48:55 minikube kubelet[2765]: E0312 18:48:55.303900    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:49:09 minikube kubelet[2765]: E0312 18:49:09.302000    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:49:24 minikube kubelet[2765]: E0312 18:49:24.301458    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:49:39 minikube kubelet[2765]: E0312 18:49:39.301907    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:49:53 minikube kubelet[2765]: E0312 18:49:53.300949    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:50:05 minikube kubelet[2765]: E0312 18:50:05.301491    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:50:20 minikube kubelet[2765]: E0312 18:50:20.301978    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:50:33 minikube kubelet[2765]: E0312 18:50:33.301920    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:50:48 minikube kubelet[2765]: E0312 18:50:48.301895    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:51:06 minikube kubelet[2765]: E0312 18:51:06.685506    2765 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="go-microservice1:latest"
Mar 12 18:51:06 minikube kubelet[2765]: E0312 18:51:06.685629    2765 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="go-microservice1:latest"
Mar 12 18:51:06 minikube kubelet[2765]: E0312 18:51:06.685906    2765 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:go-microservice1,Image:go-microservice1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8080,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-znzqb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod go-microservice1-765c877587-s57j8_default(940c035a-14fa-4910-9a8f-a37bf8753cda): ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
Mar 12 18:51:06 minikube kubelet[2765]: E0312 18:51:06.687211    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ErrImagePull: \"Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:51:20 minikube kubelet[2765]: E0312 18:51:20.301362    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:51:31 minikube kubelet[2765]: E0312 18:51:31.302066    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:51:42 minikube kubelet[2765]: E0312 18:51:42.302116    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:51:56 minikube kubelet[2765]: E0312 18:51:56.300371    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:52:08 minikube kubelet[2765]: E0312 18:52:08.301484    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:52:19 minikube kubelet[2765]: E0312 18:52:19.301751    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"
Mar 12 18:52:33 minikube kubelet[2765]: E0312 18:52:33.309365    2765 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"go-microservice1\" with ImagePullBackOff: \"Back-off pulling image \\\"go-microservice1\\\": ErrImagePull: Error response from daemon: pull access denied for go-microservice1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/go-microservice1-765c877587-s57j8" podUID="940c035a-14fa-4910-9a8f-a37bf8753cda"


==> storage-provisioner [c2e08d44fdf0] <==
I0312 18:33:41.176118       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0312 18:34:11.182617       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout


==> storage-provisioner [d8c35f7e85d7] <==
I0312 18:34:11.835575       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0312 18:34:11.846307       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0312 18:34:11.846349       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0312 18:34:11.858143       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0312 18:34:11.858362       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_f5df443a-4a16-4092-a05a-7d077890ef77!
I0312 18:34:11.858285       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"3ebf39b9-e92f-40fe-a656-337eb48f5dca", APIVersion:"v1", ResourceVersion:"424", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_f5df443a-4a16-4092-a05a-7d077890ef77 became leader
I0312 18:34:11.959291       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_f5df443a-4a16-4092-a05a-7d077890ef77!

